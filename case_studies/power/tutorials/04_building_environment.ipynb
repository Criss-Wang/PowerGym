{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tutorial 4: Building the Environment\n\n**Goal:** Create a PettingZoo-compatible environment using HERON's adapter.\n\n**Time:** ~15 minutes\n\n---\n\n## The Environment's Role\n\nThe environment **orchestrates** agents and **interfaces** with RL frameworks:\n\n```\nRLlib / StableBaselines3\n    │\n    └── PettingZoo API (step, reset, observe)\n            │\n            └── HERON Environment (PettingZooParallelEnv)\n                    │\n                    ├── Microgrid 0 (CoordinatorAgent)\n                    │   ├── Battery (FieldAgent)\n                    │   └── Generator (FieldAgent)\n                    ├── Microgrid 1 ...\n                    └── Microgrid 2 ...\n```\n\n**Key pattern:** Use `PettingZooParallelEnv` (not raw `ParallelEnv`) to get HERON features."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Our Simple Agents\n",
    "\n",
    "First, let's recreate our agents from Tutorial 3 in a compact form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from heron.core.feature import FeatureProvider\n",
    "from heron.core.state import FieldAgentState\n",
    "from heron.core.action import Action\n",
    "from heron.agents.field_agent import FieldAgent\n",
    "from heron.agents.coordinator_agent import CoordinatorAgent\n",
    "from heron.protocols.vertical import SetpointProtocol\n",
    "\n",
    "\n",
    "# === Features ===\n",
    "@dataclass\n",
    "class BatterySOC(FeatureProvider):\n",
    "    visibility = ['owner', 'upper_level']\n",
    "    soc: float = 0.5\n",
    "    \n",
    "    def vector(self) -> np.ndarray:\n",
    "        return np.array([self.soc], dtype=np.float32)\n",
    "    \n",
    "    def names(self): return ['soc']\n",
    "    def to_dict(self): return {'soc': self.soc}\n",
    "    @classmethod\n",
    "    def from_dict(cls, d): return cls(**d)\n",
    "    def set_values(self, **kw): \n",
    "        if 'soc' in kw: self.soc = float(kw['soc'])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenOutput(FeatureProvider):\n",
    "    visibility = ['owner', 'upper_level', 'system']\n",
    "    p_mw: float = 0.0\n",
    "    p_max: float = 5.0\n",
    "    \n",
    "    def vector(self) -> np.ndarray:\n",
    "        return np.array([self.p_mw / self.p_max], dtype=np.float32)\n",
    "    \n",
    "    def names(self): return ['p_norm']\n",
    "    def to_dict(self): return {'p_mw': self.p_mw, 'p_max': self.p_max}\n",
    "    @classmethod\n",
    "    def from_dict(cls, d): return cls(**d)\n",
    "    def set_values(self, **kw): \n",
    "        if 'p_mw' in kw: self.p_mw = float(kw['p_mw'])\n",
    "\n",
    "\n",
    "# === Field Agents ===\n",
    "class SimpleBattery(FieldAgent):\n",
    "    def __init__(self, agent_id: str, capacity: float = 2.0, upstream_id: str = None):\n",
    "        self.capacity = capacity\n",
    "        super().__init__(agent_id=agent_id, upstream_id=upstream_id, config={'name': agent_id})\n",
    "        self.state = FieldAgentState(owner_id=agent_id, owner_level=1)\n",
    "        self.state.register_feature('soc', BatterySOC(soc=0.5))\n",
    "        self.action_space = Box(-1, 1, (1,), np.float32)\n",
    "        self.observation_space = Box(-np.inf, np.inf, (1,), np.float32)\n",
    "    \n",
    "    def observe(self, gs=None): return self.state.vector()\n",
    "    def step(self, action, dt=1.0):\n",
    "        power = float(action[0]) * 0.5  # Max 0.5 MW\n",
    "        soc = self.state.features['soc'].soc + power * dt / self.capacity\n",
    "        self.state.features['soc'].soc = np.clip(soc, 0.1, 0.9)\n",
    "        return {'power_mw': power, 'soc': self.state.features['soc'].soc}\n",
    "    def reset(self, seed=None):\n",
    "        self.state.features['soc'].soc = 0.5\n",
    "        return self.observe()\n",
    "\n",
    "\n",
    "class SimpleGen(FieldAgent):\n",
    "    def __init__(self, agent_id: str, p_max: float = 5.0, cost: float = 50.0, upstream_id: str = None):\n",
    "        self.p_max = p_max\n",
    "        self.cost = cost\n",
    "        super().__init__(agent_id=agent_id, upstream_id=upstream_id, config={'name': agent_id})\n",
    "        self.state = FieldAgentState(owner_id=agent_id, owner_level=1)\n",
    "        self.state.register_feature('output', GenOutput(p_mw=0, p_max=p_max))\n",
    "        self.action_space = Box(0, 1, (1,), np.float32)\n",
    "        self.observation_space = Box(-np.inf, np.inf, (1,), np.float32)\n",
    "    \n",
    "    def observe(self, gs=None): return self.state.vector()\n",
    "    def step(self, action, dt=1.0):\n",
    "        p = float(action[0]) * self.p_max\n",
    "        self.state.features['output'].p_mw = p\n",
    "        return {'power_mw': p, 'cost': p * dt * self.cost}\n",
    "    def reset(self, seed=None):\n",
    "        self.state.features['output'].p_mw = 0\n",
    "        return self.observe()\n",
    "\n",
    "\n",
    "# === Coordinator Agent (Microgrid) ===\n",
    "class SimpleMicrogrid(CoordinatorAgent):\n",
    "    def __init__(self, agent_id: str, load: float = 3.0, upstream_id: str = None):\n",
    "        self.load = load\n",
    "        super().__init__(agent_id=agent_id, upstream_id=upstream_id, protocol=SetpointProtocol())\n",
    "        \n",
    "        # Create subordinates\n",
    "        self.battery = SimpleBattery(f'{agent_id}_bat', upstream_id=agent_id)\n",
    "        self.gen = SimpleGen(f'{agent_id}_gen', upstream_id=agent_id)\n",
    "        self.subordinates = {self.battery.agent_id: self.battery, self.gen.agent_id: self.gen}\n",
    "        \n",
    "        # Spaces: observe [bat_soc, gen_out, load], act [bat_setpoint, gen_setpoint]\n",
    "        self.observation_space = Box(-np.inf, np.inf, (3,), np.float32)\n",
    "        self.action_space = Box(np.array([-1, 0]), np.array([1, 1]), dtype=np.float32)\n",
    "    \n",
    "    def observe(self, gs=None) -> np.ndarray:\n",
    "        bat_soc = self.battery.state.features['soc'].soc\n",
    "        gen_out = self.gen.state.features['output'].p_mw / self.gen.p_max\n",
    "        return np.array([bat_soc, gen_out, self.load / 10], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action: np.ndarray, dt: float = 1.0) -> Dict:\n",
    "        bat_res = self.battery.step(action[0:1], dt)\n",
    "        gen_res = self.gen.step(action[1:2], dt)\n",
    "        net_power = gen_res['power_mw'] - bat_res['power_mw']\n",
    "        imbalance = abs(self.load - net_power)\n",
    "        return {'net_power': net_power, 'imbalance': imbalance, 'cost': gen_res['cost']}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.battery.reset(seed)\n",
    "        self.gen.reset(seed)\n",
    "        return self.observe()\n",
    "\n",
    "\n",
    "print(\"Agents defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Build the Multi-Agent Environment\n\nHERON provides `PettingZooParallelEnv` - an adapter that combines:\n- PettingZoo's `ParallelEnv` interface (for RL framework compatibility)\n- HERON's `HeronEnvCore` mixin (for agent management, event-driven execution)\n\n**Why use HERON's adapter instead of raw ParallelEnv?**\n- Built-in agent registration (`register_agent`, `register_agents`)\n- Event-driven execution support (`setup_event_driven`, `run_event_driven`)\n- Message broker integration for distributed mode\n- SystemAgent/ProxyAgent support\n- Helper methods for space initialization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from heron.envs.adapters import PettingZooParallelEnv\nfrom gymnasium.spaces import Dict as DictSpace\n\n\nclass SimpleMultiMicrogridEnv(PettingZooParallelEnv):\n    \"\"\"A simple multi-agent environment with 3 microgrids.\n    \n    This demonstrates the key HERON patterns:\n    - Inheriting from PettingZooParallelEnv (not raw ParallelEnv)\n    - Using register_agent() for HERON agent management\n    - PettingZoo compatibility\n    - Reward shaping for cooperation\n    \"\"\"\n    \n    metadata = {'render_modes': ['human'], 'name': 'simple_microgrids_v0'}\n    \n    def __init__(\n        self,\n        num_microgrids: int = 3,\n        max_steps: int = 96,  # 4 days at hourly steps\n        share_reward: bool = True,\n        penalty: float = 10.0,\n    ):\n        \"\"\"Initialize environment.\n        \n        Args:\n            num_microgrids: Number of microgrid agents\n            max_steps: Episode length\n            share_reward: If True, all agents get same reward (encourages cooperation)\n            penalty: Penalty coefficient for imbalance\n        \"\"\"\n        # Initialize HERON's PettingZoo adapter\n        super().__init__(env_id=\"simple_microgrids\")\n        \n        self.num_microgrids = num_microgrids\n        self.max_steps = max_steps\n        self.share_reward = share_reward\n        self.penalty = penalty\n        \n        # Create and register agents using HERON's agent management\n        self.agents_dict: Dict[str, SimpleMicrogrid] = {}\n        loads = [3.0, 4.0, 2.5]  # Different loads for each microgrid\n        \n        for i in range(num_microgrids):\n            agent_id = f'mg_{i}'\n            agent = SimpleMicrogrid(\n                agent_id=agent_id,\n                load=loads[i % len(loads)],\n                upstream_id='dso'\n            )\n            self.agents_dict[agent_id] = agent\n            # Register with HERON (enables event-driven execution, message broker, etc.)\n            self.register_agent(agent)\n        \n        # Setup PettingZoo required attributes using HERON helpers\n        self._set_agent_ids(list(self.agents_dict.keys()))\n        self._init_spaces(\n            action_spaces={aid: agent.action_space for aid, agent in self.agents_dict.items()},\n            observation_spaces={aid: agent.observation_space for aid, agent in self.agents_dict.items()},\n        )\n        \n        # State\n        self._step_count = 0\n        self._cumulative_cost = 0.0\n    \n    # observation_spaces and action_spaces are now handled by PettingZooParallelEnv\n    # We just need to provide the property aliases for compatibility\n    @property\n    def observation_space(self) -> Dict[str, Box]:\n        return self.observation_spaces\n    \n    @property\n    def action_space(self) -> Dict[str, Box]:\n        return self.action_spaces\n    \n    def reset(\n        self, \n        seed: Optional[int] = None,\n        options: Optional[Dict] = None\n    ) -> Tuple[Dict[str, np.ndarray], Dict[str, Dict]]:\n        \"\"\"Reset environment and all agents.\n        \n        Returns:\n            observations: Dict of agent_id -> observation\n            infos: Dict of agent_id -> info dict\n        \"\"\"\n        self._step_count = 0\n        self._cumulative_cost = 0.0\n        self._agents = self._possible_agents.copy()  # Reset active agents\n        \n        # Use HERON's reset_agents helper\n        self.reset_agents(seed=seed)\n        \n        # Collect observations\n        observations = {aid: agent.observe() for aid, agent in self.agents_dict.items()}\n        infos = {aid: {} for aid in self.agents}\n        \n        return observations, infos\n    \n    def step(\n        self, \n        actions: Dict[str, np.ndarray]\n    ) -> Tuple[\n        Dict[str, np.ndarray],  # observations\n        Dict[str, float],       # rewards\n        Dict[str, bool],        # terminateds\n        Dict[str, bool],        # truncateds\n        Dict[str, Dict]         # infos\n    ]:\n        \"\"\"Execute actions for all agents.\n        \n        This is where HERON's agent-centric design shines:\n        - Each agent steps independently\n        - We aggregate results for reward computation\n        \"\"\"\n        self._step_count += 1\n        self._timestep = self._step_count  # Update HERON's internal timestep\n        \n        # Step each agent\n        results = {}\n        total_cost = 0.0\n        total_imbalance = 0.0\n        \n        for agent_id, agent in self.agents_dict.items():\n            action = actions.get(agent_id, agent.action_space.sample())\n            results[agent_id] = agent.step(action)\n            total_cost += results[agent_id]['cost']\n            total_imbalance += results[agent_id]['imbalance']\n        \n        self._cumulative_cost += total_cost\n        \n        # Compute rewards\n        # Reward = -cost - penalty * imbalance\n        # Lower cost and imbalance = higher reward\n        collective_reward = -(total_cost + self.penalty * total_imbalance)\n        \n        if self.share_reward:\n            # CTDE: All agents get same reward (encourages cooperation)\n            rewards = {aid: collective_reward / self.num_microgrids for aid in self.agents}\n        else:\n            # Independent: Each agent gets its own reward\n            rewards = {\n                aid: -(results[aid]['cost'] + self.penalty * results[aid]['imbalance'])\n                for aid in self.agents\n            }\n        \n        # Get new observations\n        observations = {aid: agent.observe() for aid, agent in self.agents_dict.items()}\n        \n        # Check termination\n        done = self._step_count >= self.max_steps\n        terminateds = {aid: done for aid in self.agents}\n        terminateds['__all__'] = done\n        truncateds = {aid: False for aid in self.agents}\n        truncateds['__all__'] = False\n        \n        # Info\n        infos = {\n            aid: {\n                'cost': results[aid]['cost'],\n                'imbalance': results[aid]['imbalance'],\n                'step': self._step_count,\n            }\n            for aid in self.agents\n        }\n        \n        return observations, rewards, terminateds, truncateds, infos\n    \n    def render(self):\n        \"\"\"Print current state.\"\"\"\n        print(f\"\\nStep {self._step_count}:\")\n        for aid, agent in self.agents_dict.items():\n            bat_soc = agent.battery.state.features['soc'].soc\n            gen_out = agent.gen.state.features['output'].p_mw\n            print(f\"  {aid}: SOC={bat_soc:.2f}, Gen={gen_out:.2f}MW, Load={agent.load:.2f}MW\")\n\n\nprint(\"Environment defined using HERON's PettingZooParallelEnv!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment\n",
    "env = SimpleMultiMicrogridEnv(num_microgrids=3, max_steps=10, share_reward=True)\n",
    "\n",
    "print(\"Environment created!\")\n",
    "print(f\"Agents: {env.possible_agents}\")\n",
    "print(f\"Observation spaces: {env.observation_spaces}\")\n",
    "print(f\"Action spaces: {env.action_spaces}\")\n",
    "\n",
    "# Reset\n",
    "obs, infos = env.reset()\n",
    "print(f\"\\nInitial observations:\")\n",
    "for aid, o in obs.items():\n",
    "    print(f\"  {aid}: {o}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few steps with random actions\n",
    "print(\"Running 5 steps with random actions...\\n\")\n",
    "\n",
    "for step in range(5):\n",
    "    # Random actions\n",
    "    actions = {aid: env.action_spaces[aid].sample() for aid in env.agents}\n",
    "    \n",
    "    obs, rewards, terminateds, truncateds, infos = env.step(actions)\n",
    "    \n",
    "    env.render()\n",
    "    print(f\"  Rewards: {rewards}\")\n",
    "    print(f\"  Total cost this step: {sum(infos[aid]['cost'] for aid in env.agents):.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Understanding Key Design Decisions\n\n### Why PettingZooParallelEnv (HERON's Adapter)?\n\nHERON's `PettingZooParallelEnv` combines two things:\n1. **PettingZoo's ParallelEnv** - Works with RLlib, StableBaselines3, TorchRL\n2. **HERON's HeronEnvCore** - Agent management, event-driven execution, messaging\n\n```python\n# Raw PettingZoo (DON'T do this)\nclass MyEnv(ParallelEnv):  # No HERON features\n    pass\n\n# HERON adapter (DO this)\nclass MyEnv(PettingZooParallelEnv):  # Has HERON features\n    def __init__(self):\n        super().__init__(env_id=\"my_env\")\n        self.register_agent(agent)  # HERON agent management\n        self._set_agent_ids([...])  # HERON helper\n```\n\n### Why `register_agent()`?\n\nThis gives you:\n- Automatic agent tracking (`heron_agents`, `heron_coordinators`)\n- Event-driven execution support (Tutorial 06)\n- Message broker integration (distributed mode)\n\n### Why Shared Rewards?\n\nIn **cooperative** settings, agents should optimize collective goals:\n```python\nif self.share_reward:\n    # All agents get same reward -> learn to cooperate\n    rewards = {aid: collective_reward / num_agents for aid in agents}\n```\n\nThis is **Centralized Training with Decentralized Execution (CTDE)**.\n\n### Why Penalty for Imbalance?\n\nPower grids must balance supply and demand. The penalty:\n```python\nreward = -(cost + penalty * imbalance)\n```\nEncourages agents to coordinate generation with load."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Adding Configuration Support\n",
    "\n",
    "For production, environments should be configurable via dicts/YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example config (like what load_setup() would return)\n",
    "env_config = {\n",
    "    'num_microgrids': 3,\n",
    "    'max_steps': 96,\n",
    "    'share_reward': True,\n",
    "    'penalty': 10.0,\n",
    "    'train': True,\n",
    "    'centralized': True,\n",
    "}\n",
    "\n",
    "# Factory function for RLlib\n",
    "def create_env(config: Dict) -> SimpleMultiMicrogridEnv:\n",
    "    \"\"\"Create environment from config dict.\"\"\"\n",
    "    return SimpleMultiMicrogridEnv(\n",
    "        num_microgrids=config.get('num_microgrids', 3),\n",
    "        max_steps=config.get('max_steps', 96),\n",
    "        share_reward=config.get('share_reward', True),\n",
    "        penalty=config.get('penalty', 10.0),\n",
    "    )\n",
    "\n",
    "# Test\n",
    "env2 = create_env(env_config)\n",
    "print(f\"Created env with {env2.num_microgrids} microgrids, {env2.max_steps} max steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **Use HERON Adapters, Not Raw PettingZoo**\n   - `PettingZooParallelEnv` = ParallelEnv + HeronEnvCore\n   - Enables event-driven execution, message broker, agent management\n\n2. **Register Agents with HERON**\n   ```python\n   self.register_agent(agent)  # Enables HERON features\n   self._set_agent_ids([...])  # Setup PettingZoo attributes\n   self._init_spaces(...)      # Setup action/observation spaces\n   ```\n\n3. **PettingZoo API**\n   - `reset()` returns (observations, infos)\n   - `step(actions)` returns (obs, rewards, terminateds, truncateds, infos)\n   - All dicts keyed by agent_id\n\n4. **Shared Rewards for Cooperation**\n   - CTDE: Same reward encourages collective optimization\n   - Independent: Each agent optimizes selfishly\n\n5. **Configuration-Driven Design**\n   - Factory functions enable easy experimentation\n   - YAML/dict configs for reproducibility\n\n---\n\n**Next:** [05_training_with_rllib.ipynb](05_training_with_rllib.ipynb) - Training with MAPPO"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}