{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tutorial 5: Training with RLlib\n\n**Goal:** Train a multi-agent policy using MAPPO on our microgrid environment.\n\n**Time:** ~15 minutes (including training)\n\n---\n\n## End-to-End Workflow\n\nThis tutorial demonstrates the complete HERON + RLlib workflow:\n\n```\n1. Define Features (FeatureProvider)\n       ↓\n2. Create Agents (FieldAgent, CoordinatorAgent)\n       ↓\n3. Build Environment (PettingZooParallelEnv)\n       ↓\n4. Register with RLlib (ParallelPettingZooEnv wrapper)\n       ↓\n5. Train MAPPO (shared policy for CTDE)\n       ↓\n6. Evaluate (in synchronous mode)\n       ↓\n7. Test robustness (event-driven mode → Tutorial 06)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Complete Environment Definition\n",
    "\n",
    "Let's put everything from Tutorials 2-4 into a single, self-contained module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom gymnasium.spaces import Box\n\nfrom heron.core.feature import FeatureProvider\nfrom heron.core.state import FieldAgentState\nfrom heron.agents.field_agent import FieldAgent\nfrom heron.agents.coordinator_agent import CoordinatorAgent\nfrom heron.protocols.vertical import SetpointProtocol\nfrom heron.envs.adapters import PettingZooParallelEnv  # Use HERON adapter!\n\n\n# ============================================\n# Features\n# ============================================\n@dataclass\nclass BatterySOC(FeatureProvider):\n    \"\"\"Battery state of charge - visible to owner and coordinator.\"\"\"\n    visibility = ['owner', 'upper_level']\n    soc: float = 0.5\n    \n    def vector(self) -> np.ndarray:\n        return np.array([self.soc], dtype=np.float32)\n    def names(self): return ['soc']\n    def to_dict(self): return {'soc': self.soc}\n    @classmethod\n    def from_dict(cls, d): return cls(**d)\n    def set_values(self, **kw): \n        if 'soc' in kw: self.soc = np.clip(float(kw['soc']), 0.1, 0.9)\n\n\n@dataclass\nclass GenOutput(FeatureProvider):\n    \"\"\"Generator output - visible to owner, coordinator, and system.\"\"\"\n    visibility = ['owner', 'upper_level', 'system']\n    p_mw: float = 0.0\n    p_max: float = 5.0\n    \n    def vector(self) -> np.ndarray:\n        return np.array([self.p_mw / max(self.p_max, 1e-6)], dtype=np.float32)\n    def names(self): return ['p_norm']\n    def to_dict(self): return {'p_mw': self.p_mw, 'p_max': self.p_max}\n    @classmethod\n    def from_dict(cls, d): return cls(**d)\n    def set_values(self, **kw): \n        if 'p_mw' in kw: self.p_mw = float(kw['p_mw'])\n\n\n# ============================================\n# Field Agents (Devices)\n# ============================================\nclass SimpleBattery(FieldAgent):\n    \"\"\"Simple battery agent with charge/discharge control.\"\"\"\n    def __init__(self, agent_id: str, capacity: float = 2.0, max_power: float = 0.5, upstream_id: str = None):\n        self.capacity = capacity\n        self.max_power = max_power\n        super().__init__(agent_id=agent_id, upstream_id=upstream_id, config={'name': agent_id})\n        self.state = FieldAgentState(owner_id=agent_id, owner_level=1)\n        self.state.register_feature('soc', BatterySOC(soc=0.5))\n        self.action_space = Box(-1, 1, (1,), np.float32)\n        self.observation_space = Box(-np.inf, np.inf, (1,), np.float32)\n    \n    def observe(self, gs=None): return self.state.vector()\n    \n    def step(self, action, dt=1.0):\n        power = float(action[0]) * self.max_power\n        soc = self.state.features['soc'].soc + power * dt / self.capacity\n        self.state.features['soc'].set_values(soc=soc)\n        return {'power_mw': power, 'soc': self.state.features['soc'].soc}\n    \n    def reset(self, seed=None):\n        self.state.features['soc'].soc = 0.5\n        return self.observe()\n\n\nclass SimpleGen(FieldAgent):\n    \"\"\"Simple generator agent with power output control.\"\"\"\n    def __init__(self, agent_id: str, p_max: float = 5.0, cost: float = 50.0, upstream_id: str = None):\n        self.p_max = p_max\n        self.cost_per_mwh = cost\n        super().__init__(agent_id=agent_id, upstream_id=upstream_id, config={'name': agent_id})\n        self.state = FieldAgentState(owner_id=agent_id, owner_level=1)\n        self.state.register_feature('output', GenOutput(p_mw=0, p_max=p_max))\n        self.action_space = Box(0, 1, (1,), np.float32)\n        self.observation_space = Box(-np.inf, np.inf, (1,), np.float32)\n    \n    def observe(self, gs=None): return self.state.vector()\n    \n    def step(self, action, dt=1.0):\n        p = float(action[0]) * self.p_max\n        self.state.features['output'].set_values(p_mw=p)\n        return {'power_mw': p, 'cost': p * dt * self.cost_per_mwh}\n    \n    def reset(self, seed=None):\n        self.state.features['output'].set_values(p_mw=0)\n        return self.observe()\n\n\n# ============================================\n# Coordinator Agent (Microgrid)\n# ============================================\nclass SimpleMicrogrid(CoordinatorAgent):\n    \"\"\"Microgrid coordinator managing battery and generator.\"\"\"\n    def __init__(self, agent_id: str, load: float = 3.0, upstream_id: str = None):\n        self.load = load\n        super().__init__(agent_id=agent_id, upstream_id=upstream_id, protocol=SetpointProtocol())\n        \n        self.battery = SimpleBattery(f'{agent_id}_bat', upstream_id=agent_id)\n        self.gen = SimpleGen(f'{agent_id}_gen', upstream_id=agent_id)\n        self.subordinates = {self.battery.agent_id: self.battery, self.gen.agent_id: self.gen}\n        \n        self.observation_space = Box(-np.inf, np.inf, (3,), np.float32)\n        self.action_space = Box(np.array([-1, 0]), np.array([1, 1]), dtype=np.float32)\n    \n    def observe(self, gs=None) -> np.ndarray:\n        return np.array([\n            self.battery.state.features['soc'].soc,\n            self.gen.state.features['output'].p_mw / self.gen.p_max,\n            self.load / 10.0\n        ], dtype=np.float32)\n    \n    def step(self, action: np.ndarray, dt: float = 1.0) -> Dict:\n        bat_res = self.battery.step(action[0:1], dt)\n        gen_res = self.gen.step(action[1:2], dt)\n        net_power = gen_res['power_mw'] - bat_res['power_mw']\n        imbalance = abs(self.load - net_power)\n        return {'net_power': net_power, 'imbalance': imbalance, 'cost': gen_res['cost']}\n    \n    def reset(self, seed=None):\n        self.battery.reset(seed)\n        self.gen.reset(seed)\n        return self.observe()\n\n\n# ============================================\n# Multi-Agent Environment (using HERON adapter)\n# ============================================\nclass SimpleMicrogridEnv(PettingZooParallelEnv):\n    \"\"\"Multi-agent microgrid environment using HERON's PettingZoo adapter.\n    \n    Using PettingZooParallelEnv gives us:\n    - Built-in agent registration and management\n    - Event-driven execution support (for testing)\n    - Message broker integration (for distributed mode)\n    - Automatic space initialization helpers\n    \"\"\"\n    \n    metadata = {'render_modes': ['human'], 'name': 'simple_microgrids_v0'}\n    \n    def __init__(self, config: Dict = None):\n        # Initialize HERON's adapter first\n        super().__init__(env_id=\"simple_microgrids\")\n        \n        config = config or {}\n        self.num_microgrids = config.get('num_microgrids', 3)\n        self.max_steps = config.get('max_episode_steps', 96)\n        self.share_reward = config.get('share_reward', True)\n        self.penalty = config.get('penalty', 10.0)\n        \n        # Create agents with varying loads\n        loads = [3.0, 4.0, 2.5]\n        self.agents_dict = {}\n        for i in range(self.num_microgrids):\n            agent_id = f'mg_{i}'\n            agent = SimpleMicrogrid(agent_id, load=loads[i % len(loads)])\n            self.agents_dict[agent_id] = agent\n            # Register with HERON (enables event-driven, messaging, etc.)\n            self.register_agent(agent)\n        \n        # Setup PettingZoo attributes using HERON helpers\n        self._set_agent_ids(list(self.agents_dict.keys()))\n        self._init_spaces(\n            action_spaces={aid: a.action_space for aid, a in self.agents_dict.items()},\n            observation_spaces={aid: a.observation_space for aid, a in self.agents_dict.items()},\n        )\n        \n        self._step_count = 0\n    \n    @property\n    def observation_space(self): return self.observation_spaces\n    \n    @property\n    def action_space(self): return self.action_spaces\n    \n    def reset(self, seed=None, options=None):\n        self._step_count = 0\n        self._agents = self._possible_agents.copy()\n        \n        # Use HERON's reset helper\n        self.reset_agents(seed=seed)\n        \n        obs = {aid: a.observe() for aid, a in self.agents_dict.items()}\n        return obs, {aid: {} for aid in self.agents}\n    \n    def step(self, actions):\n        self._step_count += 1\n        self._timestep = self._step_count\n        \n        results = {}\n        total_cost, total_imbalance = 0.0, 0.0\n        \n        for aid, agent in self.agents_dict.items():\n            action = actions.get(aid, agent.action_space.sample())\n            results[aid] = agent.step(action)\n            total_cost += results[aid]['cost']\n            total_imbalance += results[aid]['imbalance']\n        \n        # Reward: minimize cost and imbalance\n        collective_reward = -(total_cost + self.penalty * total_imbalance)\n        \n        if self.share_reward:\n            rewards = {aid: collective_reward / self.num_microgrids for aid in self.agents}\n        else:\n            rewards = {aid: -(results[aid]['cost'] + self.penalty * results[aid]['imbalance']) \n                      for aid in self.agents}\n        \n        obs = {aid: a.observe() for aid, a in self.agents_dict.items()}\n        done = self._step_count >= self.max_steps\n        terminateds = {aid: done for aid in self.agents}\n        terminateds['__all__'] = done\n        truncateds = {aid: False for aid in self.agents}\n        truncateds['__all__'] = False\n        infos = {aid: {'cost': results[aid]['cost'], 'imbalance': results[aid]['imbalance']} \n                for aid in self.agents}\n        \n        return obs, rewards, terminateds, truncateds, infos\n    \n    def render(self): pass\n\n\nprint(\"Environment module ready (using HERON's PettingZooParallelEnv)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup RLlib Training\n",
    "\n",
    "Now let's configure RLlib to train MAPPO on our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True, num_cpus=2)\n",
    "\n",
    "# Environment creator function for RLlib\n",
    "def env_creator(config):\n",
    "    \"\"\"Create environment wrapped for RLlib.\"\"\"\n",
    "    env = SimpleMicrogridEnv(config)\n",
    "    return ParallelPettingZooEnv(env)\n",
    "\n",
    "# Register the environment\n",
    "register_env(\"simple_microgrids\", env_creator)\n",
    "\n",
    "print(\"RLlib initialized and environment registered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test environment to get spaces\n",
    "env_config = {\n",
    "    'num_microgrids': 3,\n",
    "    'max_episode_steps': 48,  # Shorter episodes for tutorial\n",
    "    'share_reward': True,\n",
    "    'penalty': 10.0,\n",
    "}\n",
    "\n",
    "test_env = env_creator(env_config)\n",
    "\n",
    "# Get agent IDs and spaces\n",
    "possible_agents = test_env.par_env.possible_agents\n",
    "print(f\"Agents: {possible_agents}\")\n",
    "\n",
    "# For MAPPO: All agents share one policy\n",
    "# Use first agent's spaces (they're all identical)\n",
    "first_agent = possible_agents[0]\n",
    "policies = {\n",
    "    'shared_policy': (\n",
    "        None,  # Policy class (None = default)\n",
    "        test_env.observation_space[first_agent],\n",
    "        test_env.action_space[first_agent],\n",
    "        {}  # Policy config\n",
    "    )\n",
    "}\n",
    "\n",
    "# All agents use the shared policy\n",
    "policy_mapping_fn = lambda agent_id, *args, **kwargs: 'shared_policy'\n",
    "\n",
    "print(f\"Observation space: {test_env.observation_space[first_agent]}\")\n",
    "print(f\"Action space: {test_env.action_space[first_agent]}\")\n",
    "print(f\"Policy mapping: All agents -> 'shared_policy' (MAPPO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PPO algorithm\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False,\n",
    "        enable_env_runner_and_connector_v2=False,\n",
    "    )\n",
    "    .environment(\n",
    "        env=\"simple_microgrids\",\n",
    "        env_config=env_config,\n",
    "        disable_env_checking=True,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .training(\n",
    "        lr=5e-4,  # Learning rate\n",
    "        gamma=0.99,  # Discount factor\n",
    "        lambda_=0.95,  # GAE lambda\n",
    "        entropy_coeff=0.01,  # Exploration bonus\n",
    "        clip_param=0.2,  # PPO clip parameter\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=0,  # CPU only for tutorial\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=1,  # Single worker for tutorial\n",
    "        num_envs_per_env_runner=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Additional settings\n",
    "config.train_batch_size = 500\n",
    "config.sgd_minibatch_size = 64\n",
    "config.num_sgd_iter = 5\n",
    "config.model = {\n",
    "    'fcnet_hiddens': [64, 64],  # Small network for tutorial\n",
    "    'fcnet_activation': 'relu',\n",
    "}\n",
    "config.preprocessor_pref = None\n",
    "config.enable_connectors = False\n",
    "\n",
    "print(\"PPO config ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Agent\n",
    "\n",
    "Now let's train for a few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the algorithm\n",
    "algo = config.build()\n",
    "\n",
    "print(\"Training MAPPO on Simple Microgrids...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Iter':>5} | {'Reward':>12} | {'Episodes':>10} | {'Steps':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Training loop - 10 iterations for tutorial\n",
    "num_iterations = 10\n",
    "rewards = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    \n",
    "    # Extract metrics\n",
    "    env_runners = result.get('env_runners', {})\n",
    "    reward_mean = env_runners.get('episode_reward_mean', 0)\n",
    "    episodes = env_runners.get('episodes_this_iter', 0)\n",
    "    timesteps = result.get('timesteps_total', 0)\n",
    "    \n",
    "    rewards.append(reward_mean)\n",
    "    print(f\"{i+1:5d} | {reward_mean:12.2f} | {episodes:10d} | {timesteps:10d}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Training complete! Final reward: {rewards[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(rewards)+1), rewards, 'b-o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Episode Reward')\n",
    "plt.title('MAPPO Training on Simple Microgrids')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Learned Policy\n",
    "\n",
    "Let's see how the trained agents behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation environment\n",
    "eval_env = SimpleMicrogridEnv(env_config)\n",
    "\n",
    "print(\"Evaluating trained policy...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "done = False\n",
    "total_rewards = {aid: 0.0 for aid in eval_env.agents}\n",
    "total_cost = 0.0\n",
    "total_imbalance = 0.0\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    # Get actions from trained policy\n",
    "    actions = {}\n",
    "    for agent_id, agent_obs in obs.items():\n",
    "        actions[agent_id] = algo.compute_single_action(\n",
    "            agent_obs,\n",
    "            policy_id='shared_policy',\n",
    "            explore=False\n",
    "        )\n",
    "    \n",
    "    # Step environment\n",
    "    obs, rewards, terminateds, truncateds, infos = eval_env.step(actions)\n",
    "    \n",
    "    # Accumulate metrics\n",
    "    for aid in eval_env.agents:\n",
    "        total_rewards[aid] += rewards[aid]\n",
    "        total_cost += infos[aid]['cost']\n",
    "        total_imbalance += infos[aid]['imbalance']\n",
    "    \n",
    "    done = terminateds.get('__all__', False)\n",
    "    step += 1\n",
    "    \n",
    "    # Print every 10 steps\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:3d}: Cost={total_cost/step:.2f}/step, Imbalance={total_imbalance/step:.2f}/step\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Episode complete after {step} steps\")\n",
    "print(f\"Total reward per agent: {sum(total_rewards.values())/len(total_rewards):.2f}\")\n",
    "print(f\"Avg cost per step: {total_cost/step:.2f}\")\n",
    "print(f\"Avg imbalance per step: {total_imbalance/step:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the algorithm and shutdown Ray\n",
    "algo.stop()\n",
    "ray.shutdown()\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What We Built\n\nIn this tutorial series, we built a **complete MARL case study from scratch**:\n\n| Component | What It Does | HERON Abstraction |\n|-----------|--------------|-------------------|\n| `BatterySOC`, `GenOutput` | Observable features | `FeatureProvider` |\n| `SimpleBattery`, `SimpleGen` | Device agents | `FieldAgent` |\n| `SimpleMicrogrid` | Coordinator | `CoordinatorAgent` |\n| `SimpleMicrogridEnv` | Environment | `PettingZooParallelEnv` |\n| MAPPO training | Policy learning | RLlib + PettingZoo |\n\n### Key HERON Patterns Used\n\n1. **HERON Environment Adapters**\n   - Use `PettingZooParallelEnv` (not raw `ParallelEnv`)\n   - Use `RLlibMultiAgentEnv` for direct RLlib integration\n   - Enables event-driven execution, message broker, agent management\n\n2. **Agent Registration**\n   ```python\n   self.register_agent(agent)  # HERON tracks agents\n   self._set_agent_ids([...])  # Setup PettingZoo attributes\n   self._init_spaces(...)      # Initialize spaces\n   ```\n\n3. **Visibility-based observation filtering**\n   - Features declare who can see them\n   - No manual filtering code\n\n4. **Hierarchical agent structure**\n   - Devices (L1) -> Microgrids (L2)\n   - Clear parent-child relationships\n\n5. **Protocol-based coordination**\n   - `SetpointProtocol` for hierarchical control\n   - Swappable without changing agent code\n\n### Next Steps\n\nTo build a production case study like `examples/05_mappo_training.py`:\n\n1. **Add more features**: Voltage, frequency, prices, weather\n2. **Use real physics**: PandaPower, OpenDSS, SUMO\n3. **Add event-driven mode**: See Tutorial 06 for heterogeneous tick rates, delays\n4. **Compare protocols**: Setpoint vs PriceSignal vs Consensus\n5. **Add visibility ablation**: Test different observability levels"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've built a complete MARL system from scratch:\n\n| Step | Time | What You Built |\n|------|------|----------------|\n| Features | 2 min | `BatterySOC`, `GenOutput` with visibility |\n| Agents | 3 min | `SimpleBattery`, `SimpleGen`, `SimpleMicrogrid` |\n| Environment | 2 min | `SimpleMicrogridEnv` with HERON adapter |\n| Training | 5 min | MAPPO with shared policy (CTDE) |\n\n**But wait—we trained in synchronous mode.** Will this policy work in the real world where agents have different update rates and communication delays?\n\n---\n\n**Next:** [06_event_driven_testing.ipynb](06_event_driven_testing.ipynb) — Validate policy robustness with realistic timing\n\nFor production examples:\n- `examples/05_mappo_training.py` — Full MAPPO script\n- `powergrid/` — Complete case study (14 features, 4 protocols)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}