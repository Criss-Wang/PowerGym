{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Training with RLlib\n",
    "\n",
    "**Goal:** Train a multi-agent policy using MAPPO on our microgrid environment.\n",
    "\n",
    "**Time:** ~15 minutes (including training)\n",
    "\n",
    "---\n",
    "\n",
    "## End-to-End Workflow\n",
    "\n",
    "This tutorial demonstrates the complete HERON + RLlib workflow:\n",
    "\n",
    "```\n",
    "1. Define Features (FeatureProvider with ClassVar visibility)\n",
    "       |\n",
    "2. Create Agents (FieldAgent, CoordinatorAgent, SystemAgent)\n",
    "       |\n",
    "3. Build Environment (MultiAgentEnv with system_agent)\n",
    "       |\n",
    "4. Wrap for RLlib (Gymnasium-compatible interface)\n",
    "       |\n",
    "5. Train MAPPO (shared policy for CTDE)\n",
    "       |\n",
    "6. Evaluate (in synchronous mode)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Complete Environment Definition\n",
    "\n",
    "Let's put everything from Tutorials 1-3 into a single, self-contained module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any, ClassVar, Dict, List, Optional, Sequence\n",
    "from dataclasses import dataclass, field\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from heron.core.feature import FeatureProvider\n",
    "from heron.core.action import Action\n",
    "from heron.agents.field_agent import FieldAgent\n",
    "from heron.agents.coordinator_agent import CoordinatorAgent\n",
    "from heron.agents.system_agent import SystemAgent\n",
    "from heron.envs.base import MultiAgentEnv\n",
    "from heron.scheduling.tick_config import TickConfig\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Features (with ClassVar visibility)\n",
    "# ============================================\n",
    "@dataclass(slots=True)\n",
    "class BatterySOC(FeatureProvider):\n",
    "    \"\"\"Battery state of charge - visible to owner and coordinator.\"\"\"\n",
    "    visibility: ClassVar[Sequence[str]] = ['owner', 'upper_level']\n",
    "    soc: float = 0.5\n",
    "    capacity: float = 2.0\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class GenOutput(FeatureProvider):\n",
    "    \"\"\"Generator output - visible to owner, coordinator, and system.\"\"\"\n",
    "    visibility: ClassVar[Sequence[str]] = ['owner', 'upper_level', 'system']\n",
    "    p_mw: float = 0.0\n",
    "    p_max: float = 5.0\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class SystemFrequency(FeatureProvider):\n",
    "    \"\"\"System frequency feature.\"\"\"\n",
    "    visibility: ClassVar[Sequence[str]] = ['system']\n",
    "    frequency_hz: float = 60.0\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Field Agents (L1)\n",
    "# ============================================\n",
    "class SimpleBattery(FieldAgent):\n",
    "    \"\"\"Simple battery agent with charge/discharge control.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str, capacity: float = 2.0, max_power: float = 0.5, **kwargs):\n",
    "        self.capacity = capacity\n",
    "        self.max_power = max_power\n",
    "        # Create features to pass to parent\n",
    "        features = [BatterySOC(soc=0.5, capacity=capacity)]\n",
    "        super().__init__(\n",
    "            agent_id=agent_id,\n",
    "            features=features,\n",
    "            tick_config=TickConfig.deterministic(tick_interval=1.0),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def init_action(self, features: List[FeatureProvider] = []) -> Action:\n",
    "        action = Action()\n",
    "        action.set_specs(dim_c=1, range=(np.array([-1.0]), np.array([1.0])))\n",
    "        return action\n",
    "\n",
    "    def set_action(self, action: Any, *args, **kwargs) -> None:\n",
    "        if hasattr(action, '__iter__'):\n",
    "            val = action[0] if len(action) > 0 else 0.0\n",
    "        else:\n",
    "            val = float(action)\n",
    "        self.action.set_values(val)\n",
    "\n",
    "    def set_state(self, **kwargs) -> None:\n",
    "        if 'soc' in kwargs:\n",
    "            self.state.features['BatterySOC'].soc = np.clip(float(kwargs['soc']), 0.1, 0.9)\n",
    "\n",
    "    def apply_action(self) -> None:\n",
    "        soc_feature = self.state.features['BatterySOC']\n",
    "        power = self.action.vector()[0] * self.max_power\n",
    "        new_soc = soc_feature.soc + power / self.capacity\n",
    "        self.set_state(soc=new_soc)\n",
    "\n",
    "    def compute_local_reward(self, local_state: dict) -> float:\n",
    "        return 0.0\n",
    "\n",
    "    @property\n",
    "    def cost(self) -> float:\n",
    "        return 0.0\n",
    "\n",
    "    @property\n",
    "    def safety(self) -> float:\n",
    "        soc = self.state.features['BatterySOC'].soc\n",
    "        return max(0, 0.2 - soc) + max(0, soc - 0.8)\n",
    "\n",
    "\n",
    "class SimpleGen(FieldAgent):\n",
    "    \"\"\"Simple generator agent with power output control.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str, p_max: float = 5.0, cost_per_mwh: float = 50.0, **kwargs):\n",
    "        self.p_max = p_max\n",
    "        self.cost_per_mwh = cost_per_mwh\n",
    "        features = [GenOutput(p_mw=0.0, p_max=p_max)]\n",
    "        super().__init__(\n",
    "            agent_id=agent_id,\n",
    "            features=features,\n",
    "            tick_config=TickConfig.deterministic(tick_interval=1.0),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def init_action(self, features: List[FeatureProvider] = []) -> Action:\n",
    "        action = Action()\n",
    "        action.set_specs(dim_c=1, range=(np.array([0.0]), np.array([1.0])))\n",
    "        return action\n",
    "\n",
    "    def set_action(self, action: Any, *args, **kwargs) -> None:\n",
    "        if hasattr(action, '__iter__'):\n",
    "            val = action[0] if len(action) > 0 else 0.0\n",
    "        else:\n",
    "            val = float(action)\n",
    "        self.action.set_values(val)\n",
    "\n",
    "    def set_state(self, **kwargs) -> None:\n",
    "        if 'p_mw' in kwargs:\n",
    "            self.state.features['GenOutput'].p_mw = float(kwargs['p_mw'])\n",
    "\n",
    "    def apply_action(self) -> None:\n",
    "        power = self.action.vector()[0] * self.p_max\n",
    "        self.set_state(p_mw=power)\n",
    "\n",
    "    def compute_local_reward(self, local_state: dict) -> float:\n",
    "        return 0.0\n",
    "\n",
    "    @property\n",
    "    def cost(self) -> float:\n",
    "        return self.state.features['GenOutput'].p_mw * self.cost_per_mwh\n",
    "\n",
    "    @property\n",
    "    def safety(self) -> float:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Coordinator Agent (L2)\n",
    "# ============================================\n",
    "class SimpleMicrogrid(CoordinatorAgent):\n",
    "    \"\"\"Microgrid coordinator managing battery and generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str, load: float = 3.0, **kwargs):\n",
    "        self.load = load\n",
    "        \n",
    "        # Build subordinates BEFORE calling super().__init__\n",
    "        battery = SimpleBattery(f'{agent_id}_bat', upstream_id=agent_id)\n",
    "        gen = SimpleGen(f'{agent_id}_gen', upstream_id=agent_id)\n",
    "        subordinates = {battery.agent_id: battery, gen.agent_id: gen}\n",
    "        \n",
    "        super().__init__(\n",
    "            agent_id=agent_id,\n",
    "            subordinates=subordinates,\n",
    "            tick_config=TickConfig.deterministic(tick_interval=60.0),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def cost(self) -> float:\n",
    "        return sum(s.cost for s in self.subordinates.values())\n",
    "\n",
    "    @property\n",
    "    def safety(self) -> float:\n",
    "        return sum(s.safety for s in self.subordinates.values())\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# System Agent (L3)\n",
    "# ============================================\n",
    "class SimpleGridSystem(SystemAgent):\n",
    "    \"\"\"System agent managing multiple microgrids.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str, subordinates: Dict[str, SimpleMicrogrid]):\n",
    "        features = [SystemFrequency(frequency_hz=60.0)]\n",
    "        super().__init__(\n",
    "            agent_id=agent_id,\n",
    "            features=features,\n",
    "            subordinates=subordinates,\n",
    "            tick_config=TickConfig.deterministic(tick_interval=300.0),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def cost(self) -> float:\n",
    "        return sum(mg.cost for mg in self.subordinates.values())\n",
    "\n",
    "    @property\n",
    "    def safety(self) -> float:\n",
    "        return sum(mg.safety for mg in self.subordinates.values())\n",
    "\n",
    "\n",
    "print(\"Agents defined with current HERON API!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Environment State (bridge to simulation)\n",
    "# ============================================\n",
    "@dataclass\n",
    "class SimpleEnvState:\n",
    "    \"\"\"Environment state for physics simulation.\"\"\"\n",
    "    battery_soc: Dict[str, float] = field(default_factory=dict)\n",
    "    gen_power: Dict[str, float] = field(default_factory=dict)\n",
    "    frequency: float = 60.0\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Multi-Agent Environment\n",
    "# ============================================\n",
    "class SimpleMicrogridEnv(MultiAgentEnv):\n",
    "    \"\"\"Multi-agent microgrid environment using HERON's MultiAgentEnv.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_agent: SystemAgent,\n",
    "        max_steps: int = 96,\n",
    "        share_reward: bool = True,\n",
    "        penalty: float = 10.0,\n",
    "    ):\n",
    "        self.max_steps = max_steps\n",
    "        self.share_reward = share_reward\n",
    "        self.penalty = penalty\n",
    "        self._step_count = 0\n",
    "        \n",
    "        # Store microgrid loads\n",
    "        self._mg_loads = {}\n",
    "        for mg_id, mg in system_agent.subordinates.items():\n",
    "            self._mg_loads[mg_id] = getattr(mg, 'load', 3.0)\n",
    "        \n",
    "        # Call parent init\n",
    "        super().__init__(\n",
    "            system_agent=system_agent,\n",
    "            env_id=\"simple_microgrids\",\n",
    "        )\n",
    "\n",
    "    def global_state_to_env_state(self, global_state: Dict) -> SimpleEnvState:\n",
    "        \"\"\"Convert proxy state to env state.\"\"\"\n",
    "        env_state = SimpleEnvState()\n",
    "        agent_states = global_state.get(\"agent_states\", {})\n",
    "        \n",
    "        for agent_id, state_dict in agent_states.items():\n",
    "            features = state_dict.get(\"features\", {})\n",
    "            if \"BatterySOC\" in features:\n",
    "                mg_id = \"_\".join(agent_id.split(\"_\")[:-1])\n",
    "                env_state.battery_soc[mg_id] = features[\"BatterySOC\"].get(\"soc\", 0.5)\n",
    "            if \"GenOutput\" in features:\n",
    "                mg_id = \"_\".join(agent_id.split(\"_\")[:-1])\n",
    "                env_state.gen_power[mg_id] = features[\"GenOutput\"].get(\"p_mw\", 0.0)\n",
    "        \n",
    "        return env_state\n",
    "\n",
    "    def run_simulation(self, env_state: SimpleEnvState, *args, **kwargs) -> SimpleEnvState:\n",
    "        \"\"\"Run physics simulation.\"\"\"\n",
    "        total_gen = sum(env_state.gen_power.values())\n",
    "        total_load = sum(self._mg_loads.values())\n",
    "        env_state.frequency = 60.0 + (total_gen - total_load) * 0.01\n",
    "        self._step_count += 1\n",
    "        return env_state\n",
    "\n",
    "    def env_state_to_global_state(self, env_state: SimpleEnvState) -> Dict:\n",
    "        \"\"\"Convert simulation results to global state.\"\"\"\n",
    "        agent_states = self.proxy_agent.get_serialized_agent_states()\n",
    "        for agent_id, state_dict in agent_states.items():\n",
    "            features = state_dict.get(\"features\", {})\n",
    "            if \"SystemFrequency\" in features:\n",
    "                features[\"SystemFrequency\"][\"frequency_hz\"] = env_state.frequency\n",
    "        return {\"agent_states\": agent_states}\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, **kwargs):\n",
    "        self._step_count = 0\n",
    "        return super().reset(seed=seed, **kwargs)\n",
    "\n",
    "\n",
    "print(\"Environment defined with MultiAgentEnv pattern!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Factory function to create environment\n",
    "# ============================================\n",
    "def create_env(config: Dict = None) -> SimpleMicrogridEnv:\n",
    "    \"\"\"Create environment from config dict.\"\"\"\n",
    "    config = config or {}\n",
    "    num_microgrids = config.get('num_microgrids', 3)\n",
    "    loads = [3.0, 4.0, 2.5]\n",
    "    \n",
    "    # Build agent hierarchy bottom-up\n",
    "    microgrids = {}\n",
    "    for i in range(num_microgrids):\n",
    "        mg_id = f'mg_{i}'\n",
    "        mg = SimpleMicrogrid(agent_id=mg_id, load=loads[i % len(loads)])\n",
    "        microgrids[mg_id] = mg\n",
    "    \n",
    "    system_agent = SimpleGridSystem(\n",
    "        agent_id='grid_system',\n",
    "        subordinates=microgrids\n",
    "    )\n",
    "    \n",
    "    return SimpleMicrogridEnv(\n",
    "        system_agent=system_agent,\n",
    "        max_steps=config.get('max_episode_steps', 96),\n",
    "        share_reward=config.get('share_reward', True),\n",
    "        penalty=config.get('penalty', 10.0),\n",
    "    )\n",
    "\n",
    "# Test the factory\n",
    "test_env = create_env({'num_microgrids': 3, 'max_episode_steps': 48})\n",
    "print(f\"Created env with {len(test_env._mg_loads)} microgrids\")\n",
    "print(f\"Registered agents: {list(test_env.registered_agents.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup RLlib Training\n",
    "\n",
    "Now let's configure RLlib to train MAPPO on our environment.\n",
    "\n",
    "**Note:** RLlib requires a Gymnasium-compatible interface. We'll create a simple wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Dict as DictSpace\n",
    "\n",
    "\n",
    "class RLlibEnvWrapper(gym.Env):\n",
    "    \"\"\"Wrapper to make HERON MultiAgentEnv compatible with RLlib.\n",
    "    \n",
    "    RLlib expects specific observation/action space formats.\n",
    "    This wrapper handles the conversion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        super().__init__()\n",
    "        self.env = create_env(config or {})\n",
    "        \n",
    "        # Get field agent IDs (the actual RL agents)\n",
    "        self.agent_ids = []\n",
    "        for mg_id, mg in self.env._system_agent.subordinates.items():\n",
    "            for sub_id in mg.subordinates.keys():\n",
    "                self.agent_ids.append(sub_id)\n",
    "        \n",
    "        # Define spaces for each agent\n",
    "        self._obs_space = Box(-np.inf, np.inf, (2,), np.float32)  # [soc or p_norm, load_norm]\n",
    "        self._act_space_bat = Box(np.array([-1.0]), np.array([1.0]), dtype=np.float32)\n",
    "        self._act_space_gen = Box(np.array([0.0]), np.array([1.0]), dtype=np.float32)\n",
    "        \n",
    "        # Multi-agent spaces\n",
    "        self.observation_space = DictSpace({\n",
    "            aid: self._obs_space for aid in self.agent_ids\n",
    "        })\n",
    "        self.action_space = DictSpace({\n",
    "            aid: self._act_space_bat if 'bat' in aid else self._act_space_gen\n",
    "            for aid in self.agent_ids\n",
    "        })\n",
    "        \n",
    "        self._agents = self.agent_ids.copy()\n",
    "        \n",
    "    @property\n",
    "    def possible_agents(self):\n",
    "        return self.agent_ids\n",
    "    \n",
    "    @property\n",
    "    def agents(self):\n",
    "        return self._agents\n",
    "    \n",
    "    def _get_obs(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Extract observations for each field agent.\"\"\"\n",
    "        obs = {}\n",
    "        for mg_id, mg in self.env._system_agent.subordinates.items():\n",
    "            load_norm = mg.load / 10.0\n",
    "            for sub_id, sub in mg.subordinates.items():\n",
    "                if 'BatterySOC' in sub.state.features:\n",
    "                    soc = sub.state.features['BatterySOC'].soc\n",
    "                    obs[sub_id] = np.array([soc, load_norm], dtype=np.float32)\n",
    "                elif 'GenOutput' in sub.state.features:\n",
    "                    p_norm = sub.state.features['GenOutput'].p_mw / sub.p_max\n",
    "                    obs[sub_id] = np.array([p_norm, load_norm], dtype=np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.env.reset(seed=seed)\n",
    "        self._agents = self.agent_ids.copy()\n",
    "        return self._get_obs(), {aid: {} for aid in self.agents}\n",
    "    \n",
    "    def step(self, actions: Dict[str, np.ndarray]):\n",
    "        # Step the environment\n",
    "        obs, rewards, terminateds, truncateds, infos = self.env.step(actions)\n",
    "        \n",
    "        # Convert to field agent observations and rewards\n",
    "        field_obs = self._get_obs()\n",
    "        \n",
    "        # Compute rewards per field agent\n",
    "        total_cost = self.env._system_agent.cost\n",
    "        total_safety = self.env._system_agent.safety\n",
    "        collective_reward = -(total_cost + self.env.penalty * total_safety)\n",
    "        \n",
    "        field_rewards = {\n",
    "            aid: collective_reward / len(self.agent_ids)\n",
    "            for aid in self.agent_ids\n",
    "        }\n",
    "        \n",
    "        done = self.env._step_count >= self.env.max_steps\n",
    "        field_terminateds = {aid: done for aid in self.agent_ids}\n",
    "        field_terminateds['__all__'] = done\n",
    "        field_truncateds = {aid: False for aid in self.agent_ids}\n",
    "        field_truncateds['__all__'] = False\n",
    "        field_infos = {aid: {'cost': total_cost, 'safety': total_safety} for aid in self.agent_ids}\n",
    "        \n",
    "        return field_obs, field_rewards, field_terminateds, field_truncateds, field_infos\n",
    "\n",
    "\n",
    "# Test the wrapper\n",
    "wrapped_env = RLlibEnvWrapper({'num_microgrids': 3, 'max_episode_steps': 48})\n",
    "print(f\"Agent IDs: {wrapped_env.agent_ids}\")\n",
    "print(f\"Observation space: {wrapped_env.observation_space}\")\n",
    "print(f\"Action space: {wrapped_env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True, num_cpus=2)\n",
    "\n",
    "# Environment creator function for RLlib\n",
    "def env_creator(config):\n",
    "    return RLlibEnvWrapper(config)\n",
    "\n",
    "# Register the environment\n",
    "register_env(\"simple_microgrids\", env_creator)\n",
    "\n",
    "print(\"RLlib initialized and environment registered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test environment to get spaces\n",
    "env_config = {\n",
    "    'num_microgrids': 3,\n",
    "    'max_episode_steps': 48,\n",
    "    'share_reward': True,\n",
    "    'penalty': 10.0,\n",
    "}\n",
    "\n",
    "test_env = env_creator(env_config)\n",
    "possible_agents = test_env.possible_agents\n",
    "print(f\"Agents: {possible_agents}\")\n",
    "\n",
    "# For MAPPO: All agents share one policy\n",
    "first_agent = possible_agents[0]\n",
    "policies = {\n",
    "    'shared_policy': (\n",
    "        None,\n",
    "        test_env.observation_space[first_agent],\n",
    "        test_env.action_space[first_agent],\n",
    "        {}\n",
    "    )\n",
    "}\n",
    "\n",
    "policy_mapping_fn = lambda agent_id, *args, **kwargs: 'shared_policy'\n",
    "\n",
    "print(f\"Observation space: {test_env.observation_space[first_agent]}\")\n",
    "print(f\"Action space: {test_env.action_space[first_agent]}\")\n",
    "print(f\"Policy mapping: All agents -> 'shared_policy' (MAPPO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PPO algorithm\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False,\n",
    "        enable_env_runner_and_connector_v2=False,\n",
    "    )\n",
    "    .environment(\n",
    "        env=\"simple_microgrids\",\n",
    "        env_config=env_config,\n",
    "        disable_env_checking=True,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .training(\n",
    "        lr=5e-4,\n",
    "        gamma=0.99,\n",
    "        lambda_=0.95,\n",
    "        entropy_coeff=0.01,\n",
    "        clip_param=0.2,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=0,\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=1,\n",
    "        num_envs_per_env_runner=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "config.train_batch_size = 500\n",
    "config.sgd_minibatch_size = 64\n",
    "config.num_sgd_iter = 5\n",
    "config.model = {\n",
    "    'fcnet_hiddens': [64, 64],\n",
    "    'fcnet_activation': 'relu',\n",
    "}\n",
    "config.preprocessor_pref = None\n",
    "config.enable_connectors = False\n",
    "\n",
    "print(\"PPO config ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Agent\n",
    "\n",
    "Now let's train for a few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the algorithm\n",
    "algo = config.build()\n",
    "\n",
    "print(\"Training MAPPO on Simple Microgrids...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Iter':>5} | {'Reward':>12} | {'Episodes':>10} | {'Steps':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Training loop\n",
    "num_iterations = 10\n",
    "rewards = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    \n",
    "    env_runners = result.get('env_runners', {})\n",
    "    reward_mean = env_runners.get('episode_reward_mean', 0)\n",
    "    episodes = env_runners.get('episodes_this_iter', 0)\n",
    "    timesteps = result.get('timesteps_total', 0)\n",
    "    \n",
    "    rewards.append(reward_mean)\n",
    "    print(f\"{i+1:5d} | {reward_mean:12.2f} | {episodes:10d} | {timesteps:10d}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Training complete! Final reward: {rewards[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(rewards)+1), rewards, 'b-o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Episode Reward')\n",
    "plt.title('MAPPO Training on Simple Microgrids')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Learned Policy\n",
    "\n",
    "Let's see how the trained agents behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation environment\n",
    "eval_env = RLlibEnvWrapper(env_config)\n",
    "\n",
    "print(\"Evaluating trained policy...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "done = False\n",
    "total_rewards = {aid: 0.0 for aid in eval_env.agents}\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    # Get actions from trained policy\n",
    "    actions = {}\n",
    "    for agent_id, agent_obs in obs.items():\n",
    "        actions[agent_id] = algo.compute_single_action(\n",
    "            agent_obs,\n",
    "            policy_id='shared_policy',\n",
    "            explore=False\n",
    "        )\n",
    "    \n",
    "    obs, rewards, terminateds, truncateds, infos = eval_env.step(actions)\n",
    "    \n",
    "    for aid in eval_env.agents:\n",
    "        total_rewards[aid] += rewards[aid]\n",
    "    \n",
    "    done = terminateds.get('__all__', False)\n",
    "    step += 1\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        cost = infos[eval_env.agents[0]]['cost']\n",
    "        safety = infos[eval_env.agents[0]]['safety']\n",
    "        print(f\"Step {step:3d}: Cost={cost:.2f}, Safety={safety:.2f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Episode complete after {step} steps\")\n",
    "print(f\"Total reward per agent: {sum(total_rewards.values())/len(total_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.stop()\n",
    "ray.shutdown()\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### HERON Patterns Used\n",
    "\n",
    "1. **Feature Definition with ClassVar Visibility**\n",
    "   ```python\n",
    "   @dataclass(slots=True)\n",
    "   class MyFeature(FeatureProvider):\n",
    "       visibility: ClassVar[Sequence[str]] = ['owner', 'upper_level']\n",
    "       value: float = 0.0\n",
    "   ```\n",
    "\n",
    "2. **Agent Hierarchy (Bottom-Up)**\n",
    "   ```python\n",
    "   # L1: Field agents with features in constructor\n",
    "   battery = SimpleBattery(agent_id=\"bat\", features=[...])\n",
    "   \n",
    "   # L2: Coordinators with subordinates in constructor\n",
    "   microgrid = SimpleMicrogrid(agent_id=\"mg\", subordinates={...})\n",
    "   \n",
    "   # L3: System agent with coordinators\n",
    "   system = SimpleGridSystem(agent_id=\"sys\", subordinates={...})\n",
    "   ```\n",
    "\n",
    "3. **Environment with system_agent**\n",
    "   ```python\n",
    "   env = MyEnv(system_agent=system_agent)\n",
    "   ```\n",
    "\n",
    "4. **Factory Function for RLlib**\n",
    "   ```python\n",
    "   def create_env(config):\n",
    "       # Build agents bottom-up\n",
    "       # Return wrapped environment\n",
    "   ```\n",
    "\n",
    "### Training Considerations\n",
    "\n",
    "- **Shared Policy (MAPPO)**: All agents use same neural network\n",
    "- **Cooperative Rewards**: Agents optimize collective objective\n",
    "- **Synchronous Training**: All agents act simultaneously\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [05_event_driven_testing.ipynb](05_event_driven_testing.ipynb) - Validate policy robustness with realistic timing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
