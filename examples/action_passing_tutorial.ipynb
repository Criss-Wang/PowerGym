{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERON Tutorial: Action Passing Through Agent Hierarchy\n",
    "\n",
    "This notebook demonstrates **action passing through protocols** in HERON's hierarchical multi-agent system:\n",
    "\n",
    "1. **Coordinator owns neural policy** and computes joint actions\n",
    "2. **Protocol distributes actions** to subordinate field agents\n",
    "3. **CTDE Training**: Centralized policy learning with distributed execution\n",
    "4. **Event-Driven Testing**: Asynchronous action distribution with realistic timing\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "GridSystem (L3)\n",
    "    |\n",
    "    v\n",
    "ZoneCoordinator (L2) <-- Owns policy, computes joint action\n",
    "    |                    Protocol distributes actions\n",
    "    +--> DeviceAgent (L1) - Device 1 (receives action via protocol)\n",
    "    |\n",
    "    +--> DeviceAgent (L1) - Device 2 (receives action via protocol)\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Action Protocol**: Defines how coordinator actions are distributed to subordinates\n",
    "- **VectorDecomposition**: Split joint action vector [a1, a2] → device_1 gets a1, device_2 gets a2\n",
    "- **Proportional**: Distribute scalar action proportionally based on weights\n",
    "- **Custom**: Implement your own action distribution logic\n",
    "\n",
    "**CTDE Pattern**:\n",
    "- **Training**: Coordinator observes all devices, outputs joint action, protocol distributes\n",
    "- **Execution**: Same flow, but with asynchronous timing and message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# HERON imports\n",
    "from heron.agents.field_agent import FieldAgent\n",
    "from heron.agents.coordinator_agent import CoordinatorAgent\n",
    "from heron.agents.system_agent import SystemAgent\n",
    "from heron.core.observation import Observation\n",
    "from heron.core.feature import FeatureProvider\n",
    "from heron.core.action import Action\n",
    "from heron.core.policies import Policy, obs_to_vector, vector_to_action\n",
    "from heron.envs.base import MultiAgentEnv\n",
    "from heron.protocols.base import ActionProtocol, Protocol\n",
    "from heron.protocols.vertical import VerticalProtocol\n",
    "from heron.scheduling import EventScheduler, TickConfig, JitterType\n",
    "from heron.scheduling.analysis import EventAnalyzer\n",
    "from heron.utils.typing import AgentID\n",
    "\n",
    "print(\"HERON modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Custom Action Protocol\n",
    "\n",
    "Let's implement a **ProportionalActionProtocol** that distributes a coordinator's scalar action proportionally among subordinates based on weights.\n",
    "\n",
    "**ActionProtocol Interface**:\n",
    "- `compute_action_coordination()`: Takes coordinator action + subordinate states → Returns dict of {agent_id: action}\n",
    "\n",
    "**Example**: If coordinator outputs action=10.0 with weights [0.3, 0.7]\n",
    "- device_1 receives: 10.0 × 0.3 = 3.0\n",
    "- device_2 receives: 10.0 × 0.7 = 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProportionalActionProtocol(ActionProtocol):\n",
    "    \"\"\"Distributes coordinator action proportionally based on weights.\"\"\"\n",
    "\n",
    "    def __init__(self, distribution_weights: Optional[Dict[AgentID, float]] = None):\n",
    "        self.distribution_weights = distribution_weights or {}\n",
    "\n",
    "    def compute_action_coordination(\n",
    "        self,\n",
    "        coordinator_action: Optional[Any],\n",
    "        subordinate_states: Optional[Dict[AgentID, Any]] = None,\n",
    "        coordination_messages: Optional[Dict[AgentID, Dict[str, Any]]] = None,\n",
    "        context: Optional[Dict[str, Any]] = None\n",
    "    ) -> Dict[AgentID, Any]:\n",
    "        \"\"\"Distribute coordinator action proportionally among subordinates.\"\"\"\n",
    "        if coordinator_action is None or subordinate_states is None:\n",
    "            return {sub_id: None for sub_id in (subordinate_states or {})}\n",
    "\n",
    "        # Extract action value from Action object or array\n",
    "        if hasattr(coordinator_action, 'c'):\n",
    "            total_action = float(coordinator_action.c[0]) if len(coordinator_action.c) > 0 else 0.0\n",
    "        elif isinstance(coordinator_action, np.ndarray):\n",
    "            total_action = float(coordinator_action[0]) if len(coordinator_action) > 0 else 0.0\n",
    "        else:\n",
    "            total_action = float(coordinator_action)\n",
    "\n",
    "        # Compute weights\n",
    "        sub_ids = list(subordinate_states.keys())\n",
    "        if not sub_ids:\n",
    "            return {}\n",
    "\n",
    "        if not self.distribution_weights:\n",
    "            weights = {sub_id: 1.0 / len(sub_ids) for sub_id in sub_ids}\n",
    "        else:\n",
    "            total_weight = sum(self.distribution_weights.get(sub_id, 0.0) for sub_id in sub_ids)\n",
    "            if total_weight == 0:\n",
    "                weights = {sub_id: 1.0 / len(sub_ids) for sub_id in sub_ids}\n",
    "            else:\n",
    "                weights = {\n",
    "                    sub_id: self.distribution_weights.get(sub_id, 0.0) / total_weight\n",
    "                    for sub_id in sub_ids\n",
    "                }\n",
    "\n",
    "        # Distribute action proportionally\n",
    "        actions = {}\n",
    "        for sub_id in sub_ids:\n",
    "            proportional_action = total_action * weights[sub_id]\n",
    "            actions[sub_id] = np.array([proportional_action])\n",
    "\n",
    "        # Print during event-driven execution\n",
    "        if context and \"subordinates\" in context:\n",
    "            print(f\"[ProportionalProtocol] Distributing action {total_action:.4f} -> {[(sid, f'{a[0]:.4f}') for sid, a in actions.items()]}\")\n",
    "            print(f\"  Weights: {weights}\")\n",
    "        return actions\n",
    "\n",
    "\n",
    "class ProportionalProtocol(Protocol):\n",
    "    \"\"\"Protocol with proportional action distribution.\"\"\"\n",
    "\n",
    "    def __init__(self, distribution_weights: Optional[Dict[AgentID, float]] = None):\n",
    "        from heron.protocols.base import NoCommunication\n",
    "        super().__init__(\n",
    "            communication_protocol=NoCommunication(),\n",
    "            action_protocol=ProportionalActionProtocol(distribution_weights)\n",
    "        )\n",
    "\n",
    "    def coordinate(self, coordinator_state, coordinator_action=None, info_for_subordinates=None, context=None):\n",
    "        \"\"\"Override to add debug output.\"\"\"\n",
    "        print(f\"[ProportionalProtocol.coordinate] Called with action={coordinator_action}, subordinates={list(info_for_subordinates.keys()) if info_for_subordinates else []}\")\n",
    "        return super().coordinate(coordinator_state, coordinator_action, info_for_subordinates, context)\n",
    "\n",
    "\n",
    "print(\"ProportionalActionProtocol defined!\")\n",
    "print(\"  - Distributes scalar coordinator action proportionally\")\n",
    "print(\"  - Weights can be customized per subordinate\")\n",
    "print(\"  - Default: equal distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Building Agents\n",
    "\n",
    "### 2.1 Device Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevicePowerFeature(FeatureProvider):\n",
    "    \"\"\"Power state feature for devices.\"\"\"\n",
    "    visibility = [\"public\"]\n",
    "\n",
    "    power: float = 0.0\n",
    "    capacity: float = 1.0\n",
    "\n",
    "    def vector(self):\n",
    "        \"\"\"Return feature as vector [power, capacity].\"\"\"\n",
    "        return np.array([self.power, self.capacity], dtype=np.float32)\n",
    "\n",
    "    def set_values(self, **kwargs: Any) -> None:\n",
    "        if \"power\" in kwargs:\n",
    "            self.power = np.clip(kwargs[\"power\"], -self.capacity, self.capacity)\n",
    "        if \"capacity\" in kwargs:\n",
    "            self.capacity = kwargs[\"capacity\"]\n",
    "\n",
    "\n",
    "print(\"DevicePowerFeature defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Device Agent (Level 1)\n",
    "\n",
    "**DeviceAgent** receives actions from coordinator via protocol.\n",
    "\n",
    "**Key Design**:\n",
    "- No policy at field agent level (actions come from coordinator)\n",
    "- `set_action()`: Handles actions from protocol (Action objects or arrays)\n",
    "- `set_state()`: Updates power based on received action\n",
    "- `compute_local_reward()`: Reward = minimize power deviation from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceAgent(FieldAgent):\n",
    "    \"\"\"Device field agent - receives actions from coordinator via protocol.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def power(self) -> float:\n",
    "        return self.state.features[0].power\n",
    "\n",
    "    @property\n",
    "    def capacity(self) -> float:\n",
    "        return self.state.features[0].capacity\n",
    "\n",
    "    def init_action(self, features: List[FeatureProvider] = []):\n",
    "        \"\"\"Initialize action (power control).\"\"\"\n",
    "        action = Action()\n",
    "        action.set_specs(dim_c=1, range=(np.array([-1.0]), np.array([1.0])))\n",
    "        action.set_values(np.array([0.0]))\n",
    "        return action\n",
    "\n",
    "    def compute_local_reward(self, local_state: dict) -> float:\n",
    "        \"\"\"Reward = maintain power near zero (minimize deviation).\"\"\"\n",
    "        if \"DevicePowerFeature\" in local_state:\n",
    "            feature_data = local_state[\"DevicePowerFeature\"]\n",
    "            # Handle both dict and array formats\n",
    "            if isinstance(feature_data, dict):\n",
    "                power = float(feature_data.get(\"power\", 0.0))\n",
    "            elif isinstance(feature_data, np.ndarray):\n",
    "                power = float(feature_data[0])\n",
    "            else:\n",
    "                power = float(feature_data)\n",
    "            return -power ** 2  # Penalize deviation from zero\n",
    "        return 0.0\n",
    "\n",
    "    def set_action(self, action: Any) -> None:\n",
    "        \"\"\"Set action from Action object or array.\"\"\"\n",
    "        if isinstance(action, Action):\n",
    "            # Handle dimension mismatch\n",
    "            if len(action.c) > self.action.dim_c:\n",
    "                self.action.set_values(action.c[:self.action.dim_c])\n",
    "            else:\n",
    "                self.action.set_values(c=action.c)\n",
    "        elif isinstance(action, np.ndarray):\n",
    "            if len(action) > self.action.dim_c:\n",
    "                self.action.set_values(action[:self.action.dim_c])\n",
    "            else:\n",
    "                self.action.set_values(action)\n",
    "        else:\n",
    "            self.action.set_values(np.array([action]))\n",
    "\n",
    "    def set_state(self) -> None:\n",
    "        \"\"\"Update power based on action (direct setpoint control).\"\"\"\n",
    "        new_power = self.action.c[0] * 0.5  # Scale action to reasonable power range\n",
    "        self.state.features[0].set_values(power=new_power)\n",
    "\n",
    "    def apply_action(self):\n",
    "        self.set_state()\n",
    "\n",
    "\n",
    "print(\"DeviceAgent class defined!\")\n",
    "print(\"  - Receives actions from coordinator via protocol\")\n",
    "print(\"  - Updates power based on received action\")\n",
    "print(\"  - Reward: minimize power deviation from zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Zone Coordinator (Level 2)\n",
    "\n",
    "**ZoneCoordinator** owns the neural policy and distributes actions via protocol.\n",
    "\n",
    "**Key Design**:\n",
    "- Owns policy (computes joint action for all subordinates)\n",
    "- Protocol distributes coordinator's action to field agents\n",
    "- Aggregates rewards from all subordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZoneCoordinator(CoordinatorAgent):\n",
    "    \"\"\"Coordinator - owns policy and distributes actions via protocol.\"\"\"\n",
    "\n",
    "    def compute_local_reward(self, local_state: dict) -> float:\n",
    "        \"\"\"Coordinator reward = sum of subordinate rewards.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for subordinate in self.subordinates.values():\n",
    "            if hasattr(subordinate, 'state') and subordinate.state:\n",
    "                sub_local_state = subordinate.state.to_dict()\n",
    "                if hasattr(subordinate, 'compute_local_reward'):\n",
    "                    total_reward += subordinate.compute_local_reward(sub_local_state)\n",
    "        return total_reward\n",
    "\n",
    "    def compute_action(self, obs: Any, scheduler: EventScheduler):\n",
    "        \"\"\"Override to ensure coordinate is called.\"\"\"\n",
    "        super().compute_action(obs, scheduler)\n",
    "        # Verify coordinate was called\n",
    "        if self._should_send_subordinate_actions():\n",
    "            print(f\"[Coordinator] Coordinated actions for {len(self.subordinates)} subordinates\")\n",
    "\n",
    "\n",
    "class GridSystem(SystemAgent):\n",
    "    \"\"\"System agent with periodic ticking.\"\"\"\n",
    "\n",
    "    def tick(self, scheduler: EventScheduler, current_time: float) -> None:\n",
    "        \"\"\"Override tick to schedule next tick for continuous operation.\"\"\"\n",
    "        super().tick(scheduler, current_time)\n",
    "        scheduler.schedule_agent_tick(self.agent_id)\n",
    "\n",
    "\n",
    "print(\"ZoneCoordinator and GridSystem classes defined!\")\n",
    "print(\"  - Coordinator owns policy, computes joint action\")\n",
    "print(\"  - Protocol distributes actions to devices\")\n",
    "print(\"  - System agent manages periodic ticking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Environment\n",
    "\n",
    "**ActionPassingEnv** provides the interface for action passing through protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvState:\n",
    "    def __init__(self):\n",
    "        self.timestep = 0\n",
    "\n",
    "\n",
    "class ActionPassingEnv(MultiAgentEnv):\n",
    "    \"\"\"Environment for testing action passing through protocols.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run_simulation(self, env_state: EnvState, *args, **kwargs) -> EnvState:\n",
    "        \"\"\"Simple physics simulation.\"\"\"\n",
    "        env_state.timestep += 1\n",
    "        return env_state\n",
    "\n",
    "    def env_state_to_global_state(self, env_state: EnvState) -> Dict:\n",
    "        \"\"\"Convert env state to global state.\"\"\"\n",
    "        agent_states = {}\n",
    "        for agent_id, agent in self.registered_agents.items():\n",
    "            if hasattr(agent, 'level') and agent.level == 1 and agent.state:\n",
    "                state_dict = agent.state.to_dict(include_metadata=True)\n",
    "                agent_states[agent_id] = state_dict\n",
    "        return {\"agent_states\": agent_states}\n",
    "\n",
    "    def global_state_to_env_state(self, global_state: Dict) -> EnvState:\n",
    "        \"\"\"Convert global state to env state.\"\"\"\n",
    "        return EnvState()\n",
    "\n",
    "\n",
    "print(\"ActionPassingEnv defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Neural Policy for Coordinator\n",
    "\n",
    "**CoordinatorNeuralPolicy** computes joint action for all subordinates.\n",
    "\n",
    "**Architecture**:\n",
    "- Input: Aggregated observations from all subordinates\n",
    "- Output: Joint action (dimension = number of subordinates)\n",
    "- Training: Policy gradient with advantage\n",
    "- Critic: Value function baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    \"\"\"Simple MLP for value function approximation.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = np.maximum(0, x @ self.W1 + self.b1)\n",
    "        return np.tanh(h @ self.W2 + self.b2)\n",
    "\n",
    "    def update(self, x, target, lr=0.01):\n",
    "        h = np.maximum(0, x @ self.W1 + self.b1)\n",
    "        out = np.tanh(h @ self.W2 + self.b2)\n",
    "        d_out = (out - target) * (1 - out**2)\n",
    "        self.W2 -= lr * np.outer(h, d_out)\n",
    "        self.b2 -= lr * d_out\n",
    "        d_h = d_out @ self.W2.T\n",
    "        d_h[h <= 0] = 0\n",
    "        self.W1 -= lr * np.outer(x, d_h)\n",
    "        self.b1 -= lr * d_h\n",
    "\n",
    "\n",
    "class ActorMLP(SimpleMLP):\n",
    "    \"\"\"Actor network with tanh output.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, seed=42):\n",
    "        super().__init__(input_dim, hidden_dim, output_dim, seed)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "\n",
    "    def update(self, x, action_taken, advantage, lr=0.01):\n",
    "        \"\"\"Update actor using policy gradient.\"\"\"\n",
    "        h = np.maximum(0, x @ self.W1 + self.b1)\n",
    "        current_action = np.tanh(h @ self.W2 + self.b2)\n",
    "\n",
    "        error = current_action - action_taken\n",
    "        grad_scale = advantage * (1 - current_action**2)\n",
    "\n",
    "        d_W2 = np.outer(h, grad_scale * error)\n",
    "        d_b2 = grad_scale * error\n",
    "        d_h = (grad_scale * error) @ self.W2.T\n",
    "        d_h[h <= 0] = 0\n",
    "        d_W1 = np.outer(x, d_h)\n",
    "        d_b1 = d_h\n",
    "\n",
    "        self.W2 -= lr * d_W2\n",
    "        self.b2 -= lr * d_b2.flatten()\n",
    "        self.W1 -= lr * d_W1\n",
    "        self.b1 -= lr * d_b1.flatten()\n",
    "\n",
    "\n",
    "class CoordinatorNeuralPolicy(Policy):\n",
    "    \"\"\"Neural policy for coordinator that computes joint action.\n",
    "\n",
    "    The coordinator observes all subordinate states (aggregated) and outputs\n",
    "    a single action that will be distributed to subordinates via protocol.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim=1, hidden_dim=32, seed=42):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_range = (-1.0, 1.0)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.actor = ActorMLP(obs_dim, hidden_dim, action_dim, seed)\n",
    "        self.critic = SimpleMLP(obs_dim, hidden_dim, 1, seed + 1)\n",
    "\n",
    "        self.noise_scale = 0.15\n",
    "\n",
    "    @obs_to_vector\n",
    "    @vector_to_action\n",
    "    def forward(self, obs_vec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute joint action with exploration noise.\"\"\"\n",
    "        action_mean = self.actor.forward(obs_vec)\n",
    "        action_vec = action_mean + np.random.normal(0, self.noise_scale, self.action_dim)\n",
    "        action_clipped = np.clip(action_vec, -1.0, 1.0)\n",
    "        return action_clipped\n",
    "\n",
    "    @obs_to_vector\n",
    "    @vector_to_action\n",
    "    def forward_deterministic(self, obs_vec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute joint action without exploration noise.\"\"\"\n",
    "        return self.actor.forward(obs_vec)\n",
    "\n",
    "    @obs_to_vector\n",
    "    def get_value(self, obs_vec: np.ndarray) -> float:\n",
    "        \"\"\"Estimate value of current state.\"\"\"\n",
    "        return float(self.critic.forward(obs_vec)[0])\n",
    "\n",
    "    def update(self, obs, action_taken, advantage, lr=0.01):\n",
    "        \"\"\"Update policy using policy gradient with advantage.\"\"\"\n",
    "        self.actor.update(obs, action_taken, advantage, lr)\n",
    "\n",
    "    def update_critic(self, obs, target, lr=0.01):\n",
    "        \"\"\"Update critic to better estimate values.\"\"\"\n",
    "        self.critic.update(obs, np.array([target]), lr)\n",
    "\n",
    "    def decay_noise(self, decay_rate=0.995, min_noise=0.05):\n",
    "        \"\"\"Decay exploration noise over training.\"\"\"\n",
    "        self.noise_scale = max(min_noise, self.noise_scale * decay_rate)\n",
    "\n",
    "\n",
    "print(\"CoordinatorNeuralPolicy defined!\")\n",
    "print(\"  - Observes all subordinates (aggregated)\")\n",
    "print(\"  - Outputs joint action for all subordinates\")\n",
    "print(\"  - Protocol distributes action to field agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Create Environment and Agents\n",
    "\n",
    "Now let's create the agent hierarchy with **VerticalProtocol** for vector decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create devices\n",
    "device_1 = DeviceAgent(\n",
    "    agent_id=\"device_1\",\n",
    "    features=[DevicePowerFeature(power=0.0, capacity=1.0)]\n",
    ")\n",
    "device_2 = DeviceAgent(\n",
    "    agent_id=\"device_2\",\n",
    "    features=[DevicePowerFeature(power=0.0, capacity=1.0)]\n",
    ")\n",
    "\n",
    "# Configure tick timing for event-driven mode\n",
    "field_tick_config = TickConfig.with_jitter(\n",
    "    tick_interval=2.0,\n",
    "    obs_delay=0.05,\n",
    "    act_delay=0.1,\n",
    "    msg_delay=0.05,\n",
    "    jitter_type=JitterType.GAUSSIAN,\n",
    "    jitter_ratio=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "coordinator_tick_config = TickConfig.with_jitter(\n",
    "    tick_interval=4.0,\n",
    "    obs_delay=0.1,\n",
    "    act_delay=0.15,\n",
    "    msg_delay=0.075,\n",
    "    jitter_type=JitterType.GAUSSIAN,\n",
    "    jitter_ratio=0.1,\n",
    "    seed=43\n",
    ")\n",
    "\n",
    "system_tick_config = TickConfig.with_jitter(\n",
    "    tick_interval=8.0,\n",
    "    obs_delay=0.15,\n",
    "    act_delay=0.25,\n",
    "    msg_delay=0.1,\n",
    "    jitter_type=JitterType.GAUSSIAN,\n",
    "    jitter_ratio=0.1,\n",
    "    seed=44\n",
    ")\n",
    "\n",
    "# Set tick configs\n",
    "device_1.tick_config = field_tick_config\n",
    "device_2.tick_config = field_tick_config\n",
    "\n",
    "# Coordinator with VerticalProtocol (vector decomposition)\n",
    "vertical_protocol = VerticalProtocol()\n",
    "coordinator = ZoneCoordinator(\n",
    "    agent_id=\"coordinator\",\n",
    "    subordinates={\"device_1\": device_1, \"device_2\": device_2},\n",
    "    tick_config=coordinator_tick_config,\n",
    ")\n",
    "# WORKAROUND: Set protocol after init\n",
    "coordinator.protocol = vertical_protocol\n",
    "\n",
    "system = GridSystem(\n",
    "    agent_id=\"system_agent\",\n",
    "    subordinates={\"coordinator\": coordinator},\n",
    "    tick_config=system_tick_config,\n",
    ")\n",
    "\n",
    "# Create environment\n",
    "env = ActionPassingEnv(\n",
    "    system_agent=system,\n",
    "    scheduler_config={\"start_time\": 0.0, \"time_step\": 1.0},\n",
    "    message_broker_config={\"buffer_size\": 1000, \"max_queue_size\": 100},\n",
    "    simulation_wait_interval=0.01,\n",
    ")\n",
    "\n",
    "print(\"Environment and agent hierarchy created!\")\n",
    "print(f\"  Protocol: VerticalProtocol (VectorDecomposition)\")\n",
    "print(f\"  Coordinator outputs 2-dimensional action vector [a1, a2]\")\n",
    "print(f\"  Protocol decomposes: device_1 gets a1, device_2 gets a2\")\n",
    "print(f\"\\nRegistered agents: {list(env.registered_agents.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: CTDE Training\n",
    "\n",
    "Train the coordinator's policy using CTDE:\n",
    "1. Coordinator observes all subordinates (aggregated)\n",
    "2. Coordinator computes joint action\n",
    "3. Protocol distributes actions to field agents\n",
    "4. Update policy using aggregated rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ctde(env: MultiAgentEnv, num_episodes=50, steps_per_episode=30, gamma=0.99, lr=0.02):\n",
    "    \"\"\"Train coordinator policy using CTDE with action distribution via protocol.\"\"\"\n",
    "    # Get field agent IDs\n",
    "    agent_ids = [aid for aid, agent in env.registered_agents.items() if agent.action_space is not None]\n",
    "\n",
    "    obs, _ = env.reset(seed=0)\n",
    "\n",
    "    # Observation dimension: aggregate all field agent observations\n",
    "    first_obs = obs[agent_ids[0]]\n",
    "    local_vec = list(first_obs.local.values())[0] if first_obs.local else np.array([])\n",
    "    obs_dim_per_agent = local_vec.shape[0] if hasattr(local_vec, 'shape') else 0\n",
    "    obs_dim = obs_dim_per_agent * len(agent_ids)\n",
    "\n",
    "    print(f\"Training coordinator with obs_dim={obs_dim} (aggregated from {len(agent_ids)} agents)\")\n",
    "\n",
    "    # Coordinator policy outputs joint action\n",
    "    num_subordinates = len(agent_ids)\n",
    "    coordinator_policy = CoordinatorNeuralPolicy(obs_dim=obs_dim, action_dim=num_subordinates, seed=42)\n",
    "\n",
    "    returns_history, power_history = [], []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=episode)\n",
    "\n",
    "        trajectories = {\"obs\": [], \"actions\": [], \"rewards\": []}\n",
    "        episode_return = 0.0\n",
    "        power_values = []\n",
    "\n",
    "        for step in range(steps_per_episode):\n",
    "            # Coordinator observes all subordinates\n",
    "            aggregated_obs = []\n",
    "            for aid in agent_ids:\n",
    "                obs_value = obs[aid]\n",
    "                if isinstance(obs_value, Observation):\n",
    "                    local_features = list(obs_value.local.values())\n",
    "                    obs_vec = local_features[0] if local_features else np.array([])\n",
    "                else:\n",
    "                    obs_vec = obs_value[:obs_dim_per_agent]\n",
    "                aggregated_obs.append(obs_vec)\n",
    "\n",
    "            aggregated_obs_vec = np.concatenate(aggregated_obs)\n",
    "            coordinator_observation = Observation(timestamp=step, local={\"obs\": aggregated_obs_vec})\n",
    "\n",
    "            # Coordinator computes joint action\n",
    "            coordinator_action = coordinator_policy.forward(coordinator_observation)\n",
    "\n",
    "            # Protocol distributes actions\n",
    "            coordinator_agent = env.registered_agents.get(\"coordinator\")\n",
    "            if coordinator_agent and coordinator_agent.protocol:\n",
    "                _, distributed_actions = coordinator_agent.protocol.coordinate(\n",
    "                    coordinator_state=coordinator_agent.state,\n",
    "                    coordinator_action=coordinator_action,\n",
    "                    info_for_subordinates={aid: obs[aid] for aid in agent_ids},\n",
    "                    context={\"subordinates\": coordinator_agent.subordinates}\n",
    "                )\n",
    "                actions = distributed_actions\n",
    "            else:\n",
    "                actions = {aid: coordinator_action for aid in agent_ids}\n",
    "\n",
    "            trajectories[\"obs\"].append(aggregated_obs_vec)\n",
    "            trajectories[\"actions\"].append(coordinator_action.c.copy())\n",
    "\n",
    "            obs, rewards, terminated, _, info = env.step(actions)\n",
    "\n",
    "            # Aggregate rewards\n",
    "            total_reward = sum(rewards.get(aid, 0) for aid in agent_ids)\n",
    "            trajectories[\"rewards\"].append(total_reward)\n",
    "            episode_return += total_reward\n",
    "\n",
    "            # Track power values\n",
    "            for aid in agent_ids:\n",
    "                if aid in obs:\n",
    "                    obs_value = obs[aid]\n",
    "                    if isinstance(obs_value, Observation):\n",
    "                        power_values.append(obs_value.vector()[0])\n",
    "                    else:\n",
    "                        power_values.append(obs_value[0])\n",
    "\n",
    "            if terminated.get(\"__all__\", False) or all(terminated.get(aid, False) for aid in agent_ids):\n",
    "                break\n",
    "\n",
    "        # Update coordinator policy\n",
    "        if trajectories[\"rewards\"]:\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for r in reversed(trajectories[\"rewards\"]):\n",
    "                G = r + gamma * G\n",
    "                returns.insert(0, G)\n",
    "            returns = np.array(returns)\n",
    "\n",
    "            for t in range(len(trajectories[\"obs\"])):\n",
    "                obs_t = trajectories[\"obs\"][t]\n",
    "                baseline = coordinator_policy.get_value(Observation(timestamp=t, local={\"obs\": obs_t}))\n",
    "                advantage = returns[t] - baseline\n",
    "                coordinator_policy.update(obs_t, trajectories[\"actions\"][t], advantage, lr=lr)\n",
    "                coordinator_policy.update_critic(obs_t, returns[t], lr=lr)\n",
    "\n",
    "            coordinator_policy.decay_noise()\n",
    "\n",
    "        returns_history.append(episode_return)\n",
    "        power_history.append(np.mean(power_values) if power_values else 0)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1:3d}: return={episode_return:.1f}, avg_power={np.mean(power_values) if power_values else 0:.4f}\")\n",
    "\n",
    "    return coordinator_policy, returns_history, power_history\n",
    "\n",
    "\n",
    "# Run training\n",
    "print(\"=\"*80)\n",
    "print(\"CTDE Training with Action Distribution via Protocol\")\n",
    "print(\"=\"*80)\n",
    "print(\"Coordinator computes joint action → Protocol distributes to field agents\")\n",
    "print()\n",
    "\n",
    "coordinator_policy, returns, avg_powers = train_ctde(env, num_episodes=50, steps_per_episode=30, lr=0.02)\n",
    "\n",
    "initial_avg_power = np.mean(avg_powers[:10])\n",
    "final_avg_power = np.mean(avg_powers[-10:])\n",
    "initial_return = np.mean(returns[:10])\n",
    "final_return = np.mean(returns[-10:])\n",
    "\n",
    "print(f\"\\nTraining Results:\")\n",
    "print(f\"  Initial avg power: {initial_avg_power:.4f} (return: {initial_return:.2f})\")\n",
    "print(f\"  Final avg power:   {final_avg_power:.4f} (return: {final_return:.2f})\")\n",
    "print(f\"  Return improvement: {final_return - initial_return:.2f}\")\n",
    "print(f\"  Power closer to zero: {abs(initial_avg_power) > abs(final_avg_power)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(returns, 'b-', linewidth=1, alpha=0.7)\n",
    "axes[0].plot(np.convolve(returns, np.ones(5)/5, mode='valid'), 'r-', linewidth=2, label='Moving avg')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Episode Return')\n",
    "axes[0].set_title('Training Returns')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(avg_powers, 'g-', linewidth=2)\n",
    "axes[1].axhline(y=0.0, color='red', linestyle='--', label='Target (0 power)')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Average Power')\n",
    "axes[1].set_title('Power During Training')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Event-Driven Testing\n",
    "\n",
    "Now let's test the trained policy in event-driven mode with asynchronous timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach trained policy to coordinator\n",
    "print(\"Attaching trained policy to coordinator for event-driven execution...\")\n",
    "env_coordinator = env.registered_agents.get(\"coordinator\")\n",
    "if env_coordinator:\n",
    "    env_coordinator.policy = coordinator_policy\n",
    "    print(f\"  Policy set on coordinator: {env_coordinator.policy is not None}\")\n",
    "\n",
    "# Run event-driven simulation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Event-Driven Execution with Trained Policy\")\n",
    "print(\"=\"*80)\n",
    "print(\"Coordinator uses trained policy to compute actions\")\n",
    "print(\"Protocol distributes actions to devices asynchronously\\n\")\n",
    "\n",
    "event_analyzer = EventAnalyzer(verbose=False, track_data=True)\n",
    "episode = env.run_event_driven(event_analyzer=event_analyzer, t_end=100.0)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Event-Driven Execution Statistics\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Simulation time: 0.0 → 100.0s\")\n",
    "\n",
    "print(f\"\\nEvent Counts:\")\n",
    "print(f\"  Observations requested: {event_analyzer.observation_count}\")\n",
    "print(f\"  Global state requests: {event_analyzer.global_state_count}\")\n",
    "print(f\"  Local state requests: {event_analyzer.local_state_count}\")\n",
    "print(f\"  State updates: {event_analyzer.state_update_count}\")\n",
    "print(f\"  Action results (rewards): {event_analyzer.action_result_count}\")\n",
    "\n",
    "# Count agent ticks\n",
    "print(f\"\\nAgent Tick Counts:\")\n",
    "for agent in [device_1, device_2, coordinator, system]:\n",
    "    if hasattr(agent, '_timestep') and agent._timestep > 0:\n",
    "        estimated_ticks = int(agent._timestep / agent._tick_config.tick_interval)\n",
    "        print(f\"  {agent.agent_id}: ~{estimated_ticks} ticks (final time: {agent._timestep:.1f}s)\")\n",
    "\n",
    "# Total activity\n",
    "total_activity = (event_analyzer.observation_count +\n",
    "                 event_analyzer.global_state_count +\n",
    "                 event_analyzer.local_state_count +\n",
    "                 event_analyzer.action_result_count)\n",
    "actual_end_time = max(a._timestep for a in [device_1, device_2, coordinator] if hasattr(a, '_timestep') and a._timestep > 0)\n",
    "\n",
    "print(f\"\\nActivity Summary:\")\n",
    "print(f\"  Total tracked events: {total_activity}\")\n",
    "print(f\"  Actual simulation duration: ~{actual_end_time:.1f}s\")\n",
    "print(f\"  Average activity: {total_activity / actual_end_time:.2f} events/s\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Action Passing Test Complete\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nKey Points:\")\n",
    "print(f\"  1. Coordinator owns neural policy and computes joint actions\")\n",
    "print(f\"  2. Protocol.coordinate() distributes actions to field agents\")\n",
    "print(f\"  3. Actions flow through hierarchy: System → Coordinator → Devices\")\n",
    "print(f\"  4. Event-driven execution with asynchronous timing and jitter\")\n",
    "print(f\"  5. Collected {event_analyzer.action_result_count} rewards across agents\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This tutorial demonstrated:\n",
    "\n",
    "### 1. Action Protocols\n",
    "- **ProportionalActionProtocol**: Distributes scalar action proportionally\n",
    "- **VerticalProtocol** (VectorDecomposition): Splits joint action vector among subordinates\n",
    "- Custom protocols can implement any action distribution logic\n",
    "\n",
    "### 2. Hierarchical Action Passing\n",
    "- **Coordinator owns policy**: Computes joint action for all subordinates\n",
    "- **Protocol distributes**: Actions flow from coordinator to field agents\n",
    "- **Field agents receive**: Actions via protocol (no local policies needed)\n",
    "\n",
    "### 3. CTDE Training\n",
    "- Coordinator observes all subordinates (aggregated observation)\n",
    "- Coordinator computes joint action (single policy for all agents)\n",
    "- Protocol distributes actions during training (matching execution)\n",
    "- Update policy using aggregated rewards\n",
    "\n",
    "### 4. Event-Driven Testing\n",
    "- Same protocol works in event-driven mode\n",
    "- Asynchronous action distribution with configurable delays\n",
    "- Message broker handles communication between agents\n",
    "- Realistic timing with jitter and delays\n",
    "\n",
    "### Key Design Principles\n",
    "\n",
    "**Clean Separation**:\n",
    "- Policy logic: Coordinator level\n",
    "- Action distribution: Protocol\n",
    "- Execution: Field agent level\n",
    "\n",
    "**Flexibility**:\n",
    "- Swap protocols without changing agents\n",
    "- Different protocols for different scenarios\n",
    "- Custom protocols for domain-specific distribution\n",
    "\n",
    "**Consistency**:\n",
    "- Same protocol in training and testing\n",
    "- Train once, deploy anywhere\n",
    "- Robust to timing variability\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Custom Protocols**: Implement domain-specific action distribution (e.g., priority-based, constraint-aware)\n",
    "2. **Multi-Level Coordination**: Nest coordinators with different protocols at each level\n",
    "3. **Communication Protocols**: Add message passing between agents (not just actions)\n",
    "4. **Advanced Policies**: Multi-agent RL algorithms (QMIX, MADDPG) with protocol-based action passing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
